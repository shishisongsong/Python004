作业详情：
	homework01.py是第一个作业
	homework02是第二个作业

学习总结：
	1、requests、bs4、lxml一定得用好，只要用好了这三个库，使用scrapy时就会发现爬虫真是特别好写
		1.1	bs4主要用于定位标签
		1.2 lxml主用用于定位我们想要获取的值
		1.3 这两者最好是结合一起使用，会起到事半功倍的效果

	2、做第一个作业时，对http知识有了更进一步的认识。
		2.1 网页需要验证或者登陆时，在代码里需要在header头加上cookie
		2.2 使用多个user-agent和proxy代理ip去访问，避免被反爬虫

	3、做第二个作业时，发现scrapy框架真强大
		3.1 用requests等几个库去爬虫时，很容易会被封，但是使用scrapy时，只要使用多个user-agent就很少会出现这种情况。看来是框架内容做了处理的。有空可以去研究看看
		3.2 想熟练使用scrapy，一定得把这个data flow给熟悉起来。这样编写爬虫时，就知道在哪个组件去编写对应的代码。

	ps：总结的很烂，不过总比不总结好，写多了应该会好起来😊